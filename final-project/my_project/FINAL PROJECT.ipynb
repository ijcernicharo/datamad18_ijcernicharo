{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime,os,sys,time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sc\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_2(startpath,form):\n",
    "    gd=[os.path.join(root,f) for root,dirs,files in os.walk(startpath) for f in files if f.endswith(form)]\n",
    "    return gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files available for this study: \n",
      "   >> Optional\n",
      "       ** /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Optional/subject101.dat\n",
      "       ** /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Optional/subject105.dat\n",
      "       ** /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Optional/subject106.dat\n",
      "       ** /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Optional/subject108.dat\n",
      "       ** /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Optional/subject109.dat\n",
      "   >> Protocol\n",
      "       ** /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Protocol/subject101.dat\n",
      "       ** /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Protocol/subject102.dat\n",
      "       ** /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Protocol/subject103.dat\n",
      "       ** /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Protocol/subject104.dat\n",
      "       ** /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Protocol/subject105.dat\n",
      "       ** /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Protocol/subject106.dat\n",
      "       ** /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Protocol/subject107.dat\n",
      "       ** /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Protocol/subject108.dat\n",
      "       ** /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Protocol/subject109.dat\n"
     ]
    }
   ],
   "source": [
    "files = list_files_2(actual_path,'.dat')\n",
    "files.sort()\n",
    "types = np.unique([item.split('/')[-2] for item in files])\n",
    "print('Files available for this study: ')\n",
    "counter0,counter1,optional,protocol = [0,0,[],[]]\n",
    "for item in files:\n",
    "    if types[0] in item:\n",
    "        counter0 += 1\n",
    "        if counter0 == 1:\n",
    "            print('   >> '+types[0])\n",
    "        print('       ** '+item)\n",
    "        if types[0] == 'Optional': optional.append(item)\n",
    "        if types[0] == 'Protocol': protocol.append(item)\n",
    "    if types[1] in item:\n",
    "        counter1 += 1\n",
    "        if counter1 == 1:\n",
    "            print('   >> '+types[1])\n",
    "        print('       ** '+item)\n",
    "        if types[1] == 'Optional': optional.append(item)\n",
    "        if types[1] == 'Protocol': protocol.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMU = ['Temp (°C)','AX','AY','AZ','AX2','AY2','AZ2','Giros1','Giros2','Giros3','MX','MY','MZ','O1','O2','O3','O4']\n",
    "cols = ['Time (s)','Activity ID','Heart Rate (bpm)']+['Hand '+item for item in IMU]+['Chest '+item for item in IMU]+['Ankle '+item for item in IMU]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 | File: /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Protocol/subject101.dat\n",
      "Iteration 1 | File: /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Protocol/subject102.dat\n",
      "Iteration 2 | File: /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Protocol/subject103.dat\n",
      "Iteration 3 | File: /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Protocol/subject104.dat\n",
      "Iteration 4 | File: /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Protocol/subject105.dat\n",
      "Iteration 5 | File: /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Protocol/subject106.dat\n",
      "Iteration 6 | File: /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Protocol/subject107.dat\n",
      "Iteration 7 | File: /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Protocol/subject108.dat\n",
      "Iteration 8 | File: /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Protocol/subject109.dat\n",
      "CPU times: user 2min 17s, sys: 13.2 s, total: 2min 30s\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "counter = 0\n",
    "for item in protocol:\n",
    "    print('Iteration '+str(counter)+' | File: '+item)\n",
    "    data = pd.read_csv(item, sep=r'\\s{1,}', engine='python', header=None)\n",
    "    data.columns = cols\n",
    "    data['User ID'] = int(''.join([e for e in item.split('/')[-1].split('.')[0] if e.isnumeric()]))\n",
    "    if counter == 0:\n",
    "        final_data = data\n",
    "    else:\n",
    "        final_data = pd.concat([final_data,data])\n",
    "        final_data = final_data.reset_index(drop=True)\n",
    "    del data\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 | File: /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Optional/subject101.dat\n",
      "Iteration 1 | File: /home/ivan/Desktop/IRONHACK_DATA_ANALYTICS/madrid-oct-2018/final-project/my_project/Optional/subject105.dat\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "counter = 0\n",
    "for item in optional:\n",
    "    print('Iteration '+str(counter)+' | File: '+item)\n",
    "    data = pd.read_csv(item, sep=r'\\s{1,}', engine='python', header=None)\n",
    "    data.columns = cols\n",
    "    data['User ID'] = int(''.join([e for e in item.split('/')[-1].split('.')[0] if e.isnumeric()]))\n",
    "    if counter == 0:\n",
    "        final_data_optional = data\n",
    "    else:\n",
    "        final_data_optional = pd.concat([final_data_optional,data])\n",
    "        final_data_optional = final_data_optional.reset_index(drop=True)\n",
    "    del data\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(final_data.head())\n",
    "display(final_data.describe())\n",
    "display(final_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(final_data_optional.head())\n",
    "display(final_data_optional.describe())\n",
    "display(final_data_optional.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Step > DELETE O1, O2 and O3 Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cols = [e for e in final_data.columns.tolist() if not 'O' in e]\n",
    "df = final_data[final_cols]\n",
    "del final_data\n",
    "\n",
    "final_cols_op = [e for e in final_data_optional.columns.tolist() if not 'O' in e]\n",
    "df_op = final_data_optional[final_cols_op]\n",
    "del final_data_optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets check the missing data by USER ID in % For Protocol Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "missing = []\n",
    "for item in df['User ID'].unique():\n",
    "    a = df[df['User ID'] == item]\n",
    "    print('User ID: ',item)\n",
    "    b = a.isna().sum()\n",
    "    if counter == 0:\n",
    "        ind = b.index.tolist()\n",
    "    missing.append(np.round(b.values*100/len(a),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df_user = pd.DataFrame(np.asarray(missing).T,columns=df['User ID'].unique(),index=ind)\n",
    "display(missing_df_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets check the missing values by Activity ID For Protocol Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "missing = []\n",
    "act_list = df['Activity ID'].unique()\n",
    "act_list.sort()\n",
    "for item in act_list:\n",
    "    a = df[df['Activity ID'] == item]\n",
    "    print('Activity ID: ',item)\n",
    "    b = a.isna().sum()\n",
    "    if counter == 0:\n",
    "        ind = b.index.tolist()\n",
    "    missing.append(np.round(b.values*100/len(a),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "missing_df_act = pd.DataFrame(np.asarray(missing).T,columns=act_list,index=ind)\n",
    "display(missing_df_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOW FOR OPTIONAL FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USER ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "missing = []\n",
    "for item in df_op['User ID'].unique():\n",
    "    a = df_op[df_op['User ID'] == item]\n",
    "    print('User ID: ',item)\n",
    "    b = a.isna().sum()\n",
    "    if counter == 0:\n",
    "        ind = b.index.tolist()\n",
    "    missing.append(np.round(b.values*100/len(a),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df_user_op = pd.DataFrame(np.asarray(missing).T,columns=df_op['User ID'].unique(),index=ind)\n",
    "display(missing_df_user_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACTIVITY ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "missing = []\n",
    "act_list_op = df_op['Activity ID'].unique()\n",
    "act_list_op.sort()\n",
    "for item in act_list_op:\n",
    "    a = df_op[df_op['Activity ID'] == item]\n",
    "    print('Activity ID: ',item)\n",
    "    b = a.isna().sum()\n",
    "    if counter == 0:\n",
    "        ind = b.index.tolist()\n",
    "    missing.append(np.round(b.values*100/len(a),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df_act_op = pd.DataFrame(np.asarray(missing).T,columns=act_list_op,index=ind)\n",
    "display(missing_df_act_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_op[df_op['User ID']==101].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values (do not take in count Heart Rate variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0 \n",
    "cols1 = [e for e in df.columns.tolist() if e!='Heart Rate (bpm)']\n",
    "for item in cols1:\n",
    "    if counter == 0: \n",
    "        conditional = df[item].isna()\n",
    "    else:\n",
    "        a = conditional | df[item].isna()\n",
    "        conditional = a\n",
    "    counter += 1\n",
    "conditional_1 = conditional\n",
    "print('Protocol DF >> NaN rows: ',sum(conditional),' | Original rows: ',len(df),' | % of NaN rows: ',round(sum(conditional)*100/len(df),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0 \n",
    "cols2 = [e for e in df_op.columns.tolist() if e!='Heart Rate (bpm)']\n",
    "for item in cols2:\n",
    "    if counter == 0: \n",
    "        conditional = df_op[item].isna()\n",
    "    else:\n",
    "        a = conditional | df_op[item].isna()\n",
    "        conditional = a\n",
    "    counter += 1\n",
    "conditional_2 = conditional\n",
    "print('Optional DF >> NaN rows: ',sum(conditional),' | Original rows: ',len(df_op),' | % of NaN rows: ',round(sum(conditional)*100/len(df_op),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df[['Time (s)','Heart Rate (bpm)','User ID']]\n",
    "list_us = a['User ID'].unique()\n",
    "\n",
    "for item in list_us:\n",
    "    b = a[a['User ID']==item].interpolate()\n",
    "    b = b.interpolate('nearest')\n",
    "    a.loc[a['User ID']==item,:] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_us.sort()\n",
    "counter = 1\n",
    "fig = plt.figure(figsize=(10,6.5*len(list_us)))\n",
    "for item in list_us:\n",
    "    index = int(str(len(list_us))+'1'+str(counter))\n",
    "    b = a[a['User ID']==item]\n",
    "    ax = plt.subplot(index)\n",
    "    ax.plot(b['Time (s)'],b['Heart Rate (bpm)'])\n",
    "    ax.grid()\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Heart Rate (bpm)')\n",
    "    ax.set_title('User '+str(item))\n",
    "    counter += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = df_op[['Time (s)','Heart Rate (bpm)','User ID']]\n",
    "list_us = c['User ID'].unique()\n",
    "\n",
    "for item in list_us:\n",
    "    d = c[c['User ID']==item].interpolate()\n",
    "    c.loc[c['User ID']==item,:] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_us.sort()\n",
    "counter = 1\n",
    "fig = plt.figure(figsize=(10,6.5*len(list_us)))\n",
    "for item in list_us:\n",
    "    index = int(str(len(list_us))+'1'+str(counter))\n",
    "    b = c[c['User ID']==item]\n",
    "    ax = plt.subplot(index)\n",
    "    ax.plot(b['Time (s)'],b['Heart Rate (bpm)'])\n",
    "    ax.grid()\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Heart Rate (bpm)')\n",
    "    ax.set_title('User '+str(item))\n",
    "    counter += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Time Variable is interpolated I proceed to drop rows with NaN values. It is supossed to obtain the 99% of the original rows once we have calculated we have less than 1% of rows with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Heart Rate (bpm)'] = a['Heart Rate (bpm)']\n",
    "df_op['Heart Rate (bpm)'] = c['Heart Rate (bpm)'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check that in `Heart Rate (bpm)` variable we have 0 or less than 1% rows with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print('Protocol DF >> N NaN rows for Heart Rate: ',df['Heart Rate (bpm)'].isna().sum(),' | Total rows: ',len(df),' | % of NaN rows: ',round(df['Heart Rate (bpm)'].isna().sum()*100/len(df),4))\n",
    "print('Protocol DF >> NaN rows: ',sum(conditional_1),' | Original rows: ',len(df),' | % of NaN rows: ',round(sum(conditional_1)*100/len(df),2))\n",
    "print()\n",
    "print('Optional DF >> N NaN rows for Heart Rate: ',df_op['Heart Rate (bpm)'].isna().sum(),' | Total rows: ',len(df_op),' | % of NaN rows: ',round(df_op['Heart Rate (bpm)'].isna().sum()*100/len(df_op),4))\n",
    "print('Optional DF >> NaN rows: ',sum(conditional_2),' | Original rows: ',len(df_op),' | % of NaN rows: ',round(sum(conditional_2)*100/len(df_op),2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NICE! Now lets drop NAN rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nona = df.dropna()\n",
    "df_op_nona = df_op.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if we have droped many rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PROTOCOL DF > Original N of registers: ',len(df),' > After droped the nan rows: ',len(df_nona),' > Rows droped: ',len(df)-len(df_nona),' and in % ',round((len(df)-len(df_nona))*100/len(df),2))\n",
    "print('OPTIONAL DF > Original N of registers: ',len(df_op),' > After droped the nan rows: ',len(df_op_nona),' > Rows droped: ',len(df_op)-len(df_op_nona),' and in % ',round((len(df_op)-len(df_op_nona))*100/len(df_op),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MORE DROPING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: data labeled with activityID=0 should be discarded in any kind of analysis. This data mainly\n",
    "covers transient activities between performing different activities, e.g. going from one location to the\n",
    "next activity's location, or waiting for the preparation of some equipment. Also, different parts of one\n",
    "subject's recording (in the case when the data collection was aborted for some reason) was put together\n",
    "during these transient activities (noticeable by some “jumping” in the HR-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Protocol DF rows for activity 0: ',len(df_nona[df_nona['Activity ID']==0]),' > Total rows: ',len(df_nona),' in % ',round(len(df_nona[df_nona['Activity ID']==0])*100/len(df_nona),2))\n",
    "print('Optional DF rows for activity 0: ',' > Total rows: ',len(df_op_nona[df_op_nona['Activity ID']==0]),' > Total rows: ',len(df_op_nona),' in % ',round((len(df_op_nona[df_op_nona['Activity ID']==0])*100/len(df_op_nona)),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin = df_nona[df_nona['Activity ID']!=0].copy()\n",
    "df_op_fin = df_op_nona[df_op_nona['Activity ID']!=0].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And Finally lets concatenate OPTIONAL and PROTOCOL dataframes to retrieve the final dataframe to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f = pd.concat([df_fin,df_op_fin])\n",
    "df_f = df_f.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREE RAM SPACE\n",
    "del df\n",
    "del df_fin\n",
    "del df_nona\n",
    "del df_op\n",
    "del df_op_fin\n",
    "del df_op_nona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also we are going to drop USER ID Feature, is not longer needed for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f.drop('User ID',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_f.columns)\n",
    "display(df_f.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking correlations of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = abs(df_f.corr())\n",
    "# I filter the data to display only correlations over 0.65 which I stimate as high correlation between Features\n",
    "k = k[k>0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "sns.heatmap(k,cmap=plt.cm.plasma)\n",
    "plt.grid()\n",
    "plt.title('Correlation Between Features')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have observed that we have correlation between hand and chest temperature. ALso I observe that all acceletartion sensors\n",
    "# have a high collinearity with the second pair of sensors for each part of the body. Acceleration sensors indexed as 1 (or wihout index)\n",
    "# are correlated with the sensors indexed as 2. But just with the sensors of he same body place.\n",
    "# I also have collinearity between Time and Heart Rate Features, between Chest Temp, Hand Temp and Ankle Temp, between Chest magnetic Z axis \n",
    "# (Chest MZ), Chest acceleration Z axis sensors 1 and 2 (Chest AZ/AZ2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEXT STEP CONSISTS ON DROPING ALL THESE COLUMNS TO AVOID PROBLEMS WHEN WE TRY TO CLUSTERIZE AND PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_delete = ['Time (s)','Chest Temp (°C)','Chest MZ']+[item for item in df_f.columns if 'AX2' in item or 'AY2' in item or 'AZ2' in item]\n",
    "print('Features to Drop:')\n",
    "print(columns_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_f.drop(columns_to_delete,axis=1)\n",
    "print('Number of columns droped: ',len(columns_to_delete))\n",
    "print('Final number of columns: ',len(data.columns))\n",
    "print('Final number of rows: ',len(data))\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the correlations again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_d = data.corr()\n",
    "# I filter the data to display only correlations over 0.65 which I stimate as high correlation between Features\n",
    "#k_d = k_d[k_d>0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,10))\n",
    "\n",
    "sns.heatmap(abs(k_d),cmap=plt.cm.pink_r)\n",
    "plt.grid()\n",
    "plt.title('Correlation Between Features')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perfect! I have managed to delete Features with high correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I proceed to delete the older dataframe to save RAM space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And we divide the data into 2 dataframes, one with the Feature Acivity ID for further analysis and the other wwith all the data except for the Activity ID Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_activity = data['Activity ID'].copy()\n",
    "data_final = data[[item for item in data.columns if item!='Activity ID']].copy()\n",
    "display(data_activity.head())\n",
    "display(data_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the data dataframe\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets check how our data is grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our next step is to clusterize the data\n",
    "For this task I have to normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
